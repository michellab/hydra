{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wwu/anaconda3/envs/freesolv/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# General:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "# SciKit-Optimise:\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# SVM:\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdmolfiles, rdMolDescriptors\n",
    "from rdkit.Chem import SDMolSupplier, Descriptors, Crippen, Lipinski, Fragments\n",
    "from rdkit import DataStructs\n",
    "\n",
    "# Misc.:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from scipy import stats\n",
    "import statistics\n",
    "import pickle\n",
    "from mordred import Calculator, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_dGoffset_path = './absolute_dGoffset/'\n",
    "\n",
    "# dataset_path = '~/Dropbox/FreeSolv/dGlearn-FreeSolv-master/datasets/train_compiled/dGhydr_train.csv'\n",
    "offset_col_name = 'dGoffset (kcal/mol)'\n",
    "\n",
    "# set data processing configurations:\n",
    "PCA_threshold = 0.95  # Keeps n dimensions for x variance explained\n",
    "replicates = 30  # Number of replicates per subject model\n",
    "n_calls = 40  # Number of Bayesian optimisation loops for hyperparameter optimisation, 40 is best for convergence, > 60 scales to very expensive\n",
    "startpoint_BO = np.inf  # Point to consider top-performing model from (MAE/MAD); 1.0 = no improvement on test-set variance\n",
    "ensemble_size = 10  # Amount of top-scoring models to retain per fold-dataset combination\n",
    "# KFold parameters:\n",
    "n_splits = 5  # Number of K-fold splits\n",
    "random_state = 2  # Random number seed\n",
    "\n",
    "split = 'dG(hydr)'\n",
    "translated_subject = 'absolute'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_save_loc = absolute_dGoffset_path + 'train_data.csv'\n",
    "train_df = pd.read_csv(train_df_save_loc, index_col='Unnamed: 0')\n",
    "train_dr = absolute_dGoffset_path + 'train_dr/'\n",
    "\n",
    "test_df_save_loc = absolute_dGoffset_path + 'test_data.csv'\n",
    "test_dr = absolute_dGoffset_path + 'test_dr/'\n",
    "test_df = pd.read_csv(test_df_save_loc)\n",
    "\n",
    "test_df_index_save_loc = absolute_dGoffset_path + 'test_data_index.csv'\n",
    "test_df_ID = pd.read_csv(test_df_index_save_loc, index_col='Unnamed: 0')\n",
    "test_ID = test_df_ID.index.tolist()\n",
    "\n",
    "worst_best_df = pd.read_csv(absolute_dGoffset_path + 'worst_best_ligands.csv', index_col='ID')\n",
    "worst_best_ID = worst_best_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, n_splits, random_state):\n",
    "    \"\"\"KFold implementation for pandas DataFrame.\n",
    "    (https://stackoverflow.com/questions/45115964/separate-pandas-dataframe-using-sklearns-kfold)\"\"\"\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    kfolds = []\n",
    "    global offset_col_name\n",
    "    \n",
    "    for train, validate in kf.split(dataset):\n",
    "\n",
    "        training = dataset.iloc[train]\n",
    "        train_labels = training[offset_col_name]\n",
    "        train_set = training.drop(offset_col_name, axis=1)\n",
    "\n",
    "        validating = dataset.iloc[validate]\n",
    "        validate_labels = validating[offset_col_name]\n",
    "        validate_set = validating.drop(offset_col_name, axis=1)\n",
    "\n",
    "        kfolds.append(\n",
    "        [[train_set, validate_set],\n",
    "        [train_labels, validate_labels]]\n",
    "        )\n",
    "    \n",
    "    return kfolds\n",
    "\n",
    "\n",
    "kfolds = split_dataset(train_df, n_splits, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File error: Bad input file ./absolute_dGoffset/train_dr/mobley_194273.sdf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6213de6c4e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# retrieve SDFs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         train_suppl = [Chem.SDMolSupplier(train_dr + str(sdf) + '.sdf') \n\u001b[0;32m---> 12\u001b[0;31m                        for sdf in train_IDs]\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         valdtn_suppl = [Chem.SDMolSupplier(train_dr + str(sdf) + '.sdf') \n",
      "\u001b[0;32m<ipython-input-5-6213de6c4e4e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# retrieve SDFs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         train_suppl = [Chem.SDMolSupplier(train_dr + str(sdf) + '.sdf') \n\u001b[0;32m---> 12\u001b[0;31m                        for sdf in train_IDs]\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         valdtn_suppl = [Chem.SDMolSupplier(train_dr + str(sdf) + '.sdf') \n",
      "\u001b[0;31mOSError\u001b[0m: File error: Bad input file ./absolute_dGoffset/train_dr/mobley_194273.sdf"
     ]
    }
   ],
   "source": [
    "for mol in worst_best_ID:\n",
    "    \n",
    "    trgt_suppl = Chem.SDMolSupplier(test_dr + str(mol) + '.sdf')\n",
    "    trgt_fp = Chem.RDKFingerprint(trgt_suppl[0])\n",
    "    \n",
    "    for fold in kfolds:\n",
    "        train_IDs = fold[0][0].index.tolist()\n",
    "        validate_IDs = fold[0][1].index.tolist()\n",
    "        \n",
    "        # retrieve SDFs\n",
    "        train_suppl = [Chem.SDMolSupplier(train_dr + str(sdf) + '.sdf') \n",
    "                       for sdf in train_IDs]\n",
    "\n",
    "        valdtn_suppl = [Chem.SDMolSupplier(train_dr + str(sdf) + '.sdf') \n",
    "                        for sdf in validate_IDs]\n",
    "        \n",
    "        # generate fingerprints\n",
    "        train_fp = [Chem.RDKFingerprint(mol[0]) for mol in train_suppl]\n",
    "        valdtn_fp = [Chem.RDKFingerprint(mol[0]) for mol in valdtn_suppl]\n",
    "        \n",
    "        # compute similarities\n",
    "        train_similarity = [DataStructs.FingerprintSimilarity(trgt_fp, train_mol)\n",
    "                        for train_mol in train_fp]\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_num = 1\n",
    "# for fold in kfolds:\n",
    "#     train_IDs = fold[0][0].index.tolist()\n",
    "#     validate_IDs = fold[0][1].index.tolist()\n",
    "#     test_IDs = test_ID\n",
    "#     wb_IDs = worst_best_ID\n",
    "\n",
    "#     # retrieve SDFs\n",
    "#     train_suppl = [Chem.SDMolSupplier(train_dr + str(sdf) + '.sdf') \n",
    "#                    for sdf in train_IDs]\n",
    "\n",
    "#     valdtn_suppl = [Chem.SDMolSupplier(train_dr + str(sdf) + '.sdf') \n",
    "#                for sdf in validate_IDs]\n",
    "\n",
    "#     test_suppl = [Chem.SDMolSupplier(test_dr + sdf + '.sdf') \n",
    "#            for sdf in test_ID]\n",
    "    \n",
    "#     wb_suppl = [Chem.SDMolSupplier(test_dr + sdf + '.sdf')\n",
    "#                   for sdf in wb_IDs]\n",
    "\n",
    "#     # generate fingerprints\n",
    "#     train_fp = [Chem.RDKFingerprint(mol[0]) for mol in train_suppl]\n",
    "#     valdtn_fp = [Chem.RDKFingerprint(mol[0]) for mol in valdtn_suppl]\n",
    "#     test_fp = [Chem.RDKFingerprint(mol[0]) for mol in test_suppl]\n",
    "#     wb_fp = [Chem.RDKFingerprint(mol[0]) for mol in wb_suppl]\n",
    "\n",
    "#     # compare fingerprints\n",
    "#     test_train_similarity = [DataStructs.FingerprintSimilarity(test_mol, train_mol)\n",
    "#                             for test_mol in test_fp\n",
    "#                             for train_mol in train_fp]\n",
    "\n",
    "#     test_valdtn_similarity = [DataStructs.FingerprintSimilarity(test_mol, valdtn_mol)\n",
    "#                         for test_mol in test_fp\n",
    "#                         for valdtn_mol in valdtn_fp]\n",
    "    \n",
    "#     train_wb_similarity = [DataStructs.FingerprintSimilarity(test_mol, train_mol)\n",
    "#                              for test_mol in wb_fp\n",
    "#                              for train_mol in train_fp]\n",
    "\n",
    "#     valdtn_wb_similarity = [DataStructs.FingerprintSimilarity(test_mol, valdtn_mol)\n",
    "#                               for test_mol in wb_fp\n",
    "#                               for valdtn_mol in valdtn_fp]\n",
    "\n",
    "#     # density plot\n",
    "#     sns.distplot(train_wb_similarity,\n",
    "#                  hist = False,\n",
    "#                  kde = True,\n",
    "#                  kde_kws = {'linewidth': 3},\n",
    "#                  label = 'wb vs train')\n",
    "\n",
    "#     sns.distplot(valdtn_wb_similarity,\n",
    "#                  hist = False,\n",
    "#                  kde = True,\n",
    "#                  kde_kws = {'linewidth': 3},\n",
    "#                  label = 'wb vs valdtn')\n",
    "    \n",
    "# #     plt.hist(test_train_similarity, \n",
    "# #              histtype='step', \n",
    "# #              bins=list(np.linspace(0, 1, 100, endpoint=False)), \n",
    "# #              label='train vs test')\n",
    "\n",
    "# #     plt.hist(test_valdtn_similarity, \n",
    "# #              histtype='step', \n",
    "# #              bins=list(np.linspace(0, 1, 100, endpoint=False)), \n",
    "# #              label='valdtn vs test')\n",
    "\n",
    "#     title = 'Fold ' +  str(fold_num)\n",
    "    \n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Fingerprint similarity')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     fold_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
